# ðŸš€ Startup Evaluation Pipeline ðŸš€

This project is a multi-agent pipeline for evaluating startup submissions using OpenAIâ€™s GPT-4o-mini. It is designed to assess a startupâ€™s product, team, market, vision, and scalability, while enforcing guardrails for PII, bias, hallucinations, and extreme language.

---------------------

Features
	â€¢	Multi-Agent System â€” Modular agents for specific tasks (e.g., product, market, team evaluation).
	â€¢	Guardrails â€” Input/output checks for PII, bias, hallucination, extreme language, and sensitive content.
	â€¢	Automated Reports â€” Synthesizes evaluations into professional-grade summaries.
	â€¢	Improvement Suggestions â€” Offers detailed critiques on startup differentiation, vision, and scalability.
	â€¢	Risk Identification â€” Flags common startup risk factors.
	â€¢	Extendable Architecture â€” Built with modular agents and guardrail hooks.

---------------------

Setup Instructions

1. Clone the Repository

2. Create and Activate Virtual Environment

python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows

3. Install Dependencies

pip install -r requirements.txt

4. Configure Environment

Create a .env file in the root directory with the following:

OPENAI_API_KEY=your_openai_key_here
MODEL=gpt-4o-mini


---------------------

Guardrails Overview

1. pii_and_poli_guardrail (Input):
Checks for Personally Identifiable Information (PII) and political content before processing any input.

2. bias_in_product_eval (Output):
Detects bias in product evaluations, such as favoritism or unfair judgments.

3. bias_in_team_eval (Output):
Flags bias related to age, gender, or race in team assessments.

4. sens_content_guardrail (Output):
Identifies political, hateful, or otherwise sensitive content in the output.

5. hallucination_output_guardrail (Output):
Detects hallucinated or ungrounded content generated by the model.

6. extreme_language_guardrail (Output):
Flags language that is overly hyperbolic, emotional, or sensational.

7. bias_detection_guardrail (Output):
Provides general bias detection across the orchestratorâ€™s output.

---------------------

Agents Breakdown

Evaluators
1. product_eval_agent â€” Assesses product quality, defensibility, and user fit.
2. market_eval_agent â€” Evaluates market timing, size, and competitive landscape.
3. team_eval_agent â€” Reviews the leadership teamâ€™s experience and execution.


Improvement Analyzers
1. differentiation_analyzer_agent â€” Evaluates how unique the startup is.
2. vision_agent â€” Assesses the long-term vision and ambition.
3. scalability_agent â€” Examines whether the startup can grow efficiently.

Other Agents
1. risk_finder_agent â€” Flags potential business risks (e.g., solo founder, no revenue model).
2. orchestrator_agent â€” Central coordinator with input/output guardrails.

---------------------

Usage

Run the main program:

You will be prompted to enter your startup idea. Example:

A SaaS tool helping remote-first engineering teams monitor productivity using ethical AI without surveilling developers directly.

The system will process the input, evaluate it using agents, enforce guardrails, and return a multi-part analysis report.

---------------------

Example Output

Startup Overview:
This startup targets remote-first engineering teams by offering ethical productivity analytics.

Product Evaluation (Score: 4/5):
The product avoids invasive surveillance, which is a strong differentiator...

Market Evaluation (Score: 3.5/5):
Large and growing market due to global shift to remote work...

Team Evaluation (Score: 4/5):
Founders have prior experience in productivity tooling...

Risk Summary:
- High competition space
- Requires constant feature innovation


---------------------

# Critical Improvements (For future projects)

Code Clarity
	â€¢	Add consistent and descriptive docstrings to all functions and classes.
	â€¢	Reduce overly nested comment sections to clean up readability.
	â€¢	Standardize terminology used across instructions, class names, and agents.

Modularization
	â€¢	Split the monolithic script into separate modules:
	â€¢	guardrails
	â€¢	agents
	â€¢	evaluators
	â€¢	main

More Dynamic Interactions
	â€¢	Enable follow-up Q&A between the orchestrator_agent and user (e.g., clarification questions).
	â€¢	Allow multi-step input or progressive evaluation rather than a one-shot input.

Reflection Agent 
	â€¢	Create a Reflection/Critique Agent that quickly evaluates the final output for:
	â€¢	Relevance
	â€¢	Completeness
	â€¢	Efficiency
	â€¢	Use this agent to validate and fine-tune outputs before final presentation.

---------------------
Acknowledgments

This project uses the OpenAI SDK and was inspired by modular agent-based design patterns for evaluation systems.

---------------------

Contact

Have suggestions or want to collaborate? Reach out via GitHub Issues or submit a pull request!
